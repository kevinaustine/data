{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62a35af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "    <property>\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "head -20 /usr/local/hadoop/etc/hadoop/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3231be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -usage ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62291bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...] :\n",
      "  List the contents that match the specified file pattern. If path is not\n",
      "  specified, the contents of /user/<currentUser> will be listed. For a directory a\n",
      "  list of its direct children is returned (unless -d option is specified).\n",
      "  \n",
      "  Directory entries are of the form:\n",
      "  \tpermissions - userId groupId sizeOfDirectory(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) directoryName\n",
      "  \n",
      "  and file entries are of the form:\n",
      "  \tpermissions numberOfReplicas userId groupId sizeOfFile(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) fileName\n",
      "  \n",
      "    -C  Display the paths of files and directories only.\n",
      "    -d  Directories are listed as plain files.\n",
      "    -h  Formats the sizes of files in a human-readable fashion\n",
      "        rather than a number of bytes.\n",
      "    -q  Print ? instead of non-printable characters.\n",
      "    -R  Recursively list the contents of directories.\n",
      "    -t  Sort files by modification time (most recent first).\n",
      "    -S  Sort files by size.\n",
      "    -r  Reverse the order of the sort.\n",
      "    -u  Use time of last access instead of modification for\n",
      "        display and sorting.\n",
      "    -e  Display the erasure coding policy of files and directories.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2fec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-02-27 08:22 /data\n",
      "drwxr-xr-x   - hadoop supergroup          0 2023-03-22 20:31 /spark-logs\n",
      "-rw-r--r--   2 hadoop supergroup         34 2023-02-10 19:23 /text.txt\n",
      "drwxrwxr-x   - hadoop supergroup          0 2023-02-18 06:12 /tmp\n",
      "drwxr-xr-x   - hadoop supergroup          0 2023-02-18 04:35 /user\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -ls /\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bddcbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst> :\n",
      "  Copy files from the local file system into fs. Copying fails if the file already\n",
      "  exists, unless the -f flag is given.\n",
      "  Flags:\n",
      "                                                                                 \n",
      "  -p                           Preserves timestamps, ownership and the mode.     \n",
      "  -f                           Overwrites the destination if it already exists.  \n",
      "  -t <thread count>            Number of threads to be used, default is 1.       \n",
      "  -q <thread pool queue size>  Thread pool queue size to be used, default is     \n",
      "                               1024.                                             \n",
      "  -l                           Allow DataNode to lazily persist the file to disk.\n",
      "                               Forces replication factor of 1. This flag will    \n",
      "                               result in reduced durability. Use with care.      \n",
      "  -d                           Skip creation of temporary file(<dst>._COPYING_). \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79c2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\n",
      "\t[-appendToFile <localsrc> ... <dst>]\n",
      "\t[-cat [-ignoreCrc] <src> ...]\n",
      "\t[-checksum [-v] <src> ...]\n",
      "\t[-chgrp [-R] GROUP PATH...]\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
      "\t[-concat <target path> <src path> <src path> ...]\n",
      "\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] [-s] <path> ...]\n",
      "\t[-cp [-f] [-p | -p[topax]] [-d] [-t <thread count>] [-q <thread pool queue size>] <src> ... <dst>]\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
      "\t[-df [-h] [<path> ...]]\n",
      "\t[-du [-s] [-h] [-v] [-x] <path> ...]\n",
      "\t[-expunge [-immediate] [-fs <path>]]\n",
      "\t[-find <path> ... <expression> ...]\n",
      "\t[-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]\n",
      "\t[-getfacl [-R] <path>]\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
      "\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n",
      "\t[-head <file>]\n",
      "\t[-help [cmd ...]]\n",
      "\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n",
      "\t[-mkdir [-p] <path> ...]\n",
      "\t[-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n",
      "\t[-moveToLocal <src> <localdst>]\n",
      "\t[-mv <src> ... <dst>]\n",
      "\t[-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
      "\t[-stat [format] <path> ...]\n",
      "\t[-tail [-f] [-s <sleep interval>] <file>]\n",
      "\t[-test -[defswrz] <path>]\n",
      "\t[-text [-ignoreCrc] <src> ...]\n",
      "\t[-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] <path> ...]\n",
      "\t[-touchz <path> ...]\n",
      "\t[-truncate [-w] <length> <path> ...]\n",
      "\t[-usage [cmd ...]]\n",
      "\n",
      "-appendToFile <localsrc> ... <dst> :\n",
      "  Appends the contents of all the given local files to the given dst file. The dst\n",
      "  file will be created if it does not exist. If <localSrc> is -, then the input is\n",
      "  read from stdin.\n",
      "\n",
      "-cat [-ignoreCrc] <src> ... :\n",
      "  Fetch all files that match the file pattern <src> and display their content on\n",
      "  stdout.\n",
      "\n",
      "-checksum [-v] <src> ... :\n",
      "  Dump checksum information for files that match the file pattern <src> to stdout.\n",
      "  Note that this requires a round-trip to a datanode storing each block of the\n",
      "  file, and thus is not efficient to run on a large number of files. The checksum\n",
      "  of a file depends on its content, block size and the checksum algorithm and\n",
      "  parameters used for creating the file.\n",
      "\n",
      "-chgrp [-R] GROUP PATH... :\n",
      "  This is equivalent to -chown ... :GROUP ...\n",
      "\n",
      "-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH... :\n",
      "  Changes permissions of a file. This works similar to the shell's chmod command\n",
      "  with a few exceptions.\n",
      "                                                                                 \n",
      "  -R           modifies the files recursively. This is the only option currently \n",
      "               supported.                                                        \n",
      "  <MODE>       Mode is the same as mode used for the shell's command. The only   \n",
      "               letters recognized are 'rwxXt', e.g. +t,a+r,g-w,+rwx,o=r.         \n",
      "  <OCTALMODE>  Mode specifed in 3 or 4 digits. If 4 digits, the first may be 1 or\n",
      "               0 to turn the sticky bit on or off, respectively.  Unlike the     \n",
      "               shell command, it is not possible to specify only part of the     \n",
      "               mode, e.g. 754 is same as u=rwx,g=rx,o=r.                         \n",
      "  \n",
      "  If none of 'augo' is specified, 'a' is assumed and unlike the shell command, no\n",
      "  umask is applied.\n",
      "\n",
      "-chown [-R] [OWNER][:[GROUP]] PATH... :\n",
      "  Changes owner and group of a file. This is similar to the shell's chown command\n",
      "  with a few exceptions.\n",
      "                                                                                 \n",
      "  -R  modifies the files recursively. This is the only option currently          \n",
      "      supported.                                                                 \n",
      "  \n",
      "  If only the owner or group is specified, then only the owner or group is\n",
      "  modified. The owner and group names may only consist of digits, alphabet, and\n",
      "  any of [-_./@a-zA-Z0-9]. The names are case sensitive.\n",
      "  \n",
      "  WARNING: Avoid using '.' to separate user name and group though Linux allows it.\n",
      "  If user names have dots in them and you are using local file system, you might\n",
      "  see surprising results since the shell command 'chown' is used for local files.\n",
      "\n",
      "-concat <target path> <src path> <src path> ... :\n",
      "  Concatenate existing source files into the target file. Target file and source\n",
      "  files should be in the same directory.\n",
      "\n",
      "-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst> :\n",
      "  Identical to the -put command.\n",
      "\n",
      "-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst> :\n",
      "  Identical to the -get command.\n",
      "\n",
      "-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] [-s] <path> ... :\n",
      "  Count the number of directories, files and bytes under the paths\n",
      "  that match the specified file pattern.  The output columns are:\n",
      "  DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\n",
      "  or, with the -q option:\n",
      "  QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA\n",
      "        DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\n",
      "  The -h option shows file sizes in human readable format.\n",
      "  The -v option displays a header line.\n",
      "  The -x option excludes snapshots from being calculated. \n",
      "  The -t option displays quota by storage types.\n",
      "  It should be used with -q or -u option, otherwise it will be ignored.\n",
      "  If a comma-separated list of storage types is given after the -t option, \n",
      "  it displays the quota and usage for the specified types. \n",
      "  Otherwise, it displays the quota and usage for all the storage \n",
      "  types that support quota. The list of possible storage types(case insensitive):\n",
      "  ram_disk, ssd, disk and archive.\n",
      "  It can also pass the value '', 'all' or 'ALL' to specify all the storage types.\n",
      "  The -u option shows the quota and \n",
      "  the usage against the quota without the detailed content summary.The -e option\n",
      "  shows the erasure coding policy.The -s option shows snapshot counts.\n",
      "\n",
      "-cp [-f] [-p | -p[topax]] [-d] [-t <thread count>] [-q <thread pool queue size>] <src> ... <dst> :\n",
      "  Copy files that match the file pattern <src> to a destination. When copying\n",
      "  multiple files, the destination must be a directory.\n",
      "  Flags :\n",
      "                                                                                 \n",
      "  -p[topax]                    Preserve file attributes [topx] (timestamps,      \n",
      "                               ownership, permission, ACL, XAttr). If -p is      \n",
      "                               specified with no arg, then preserves timestamps, \n",
      "                               ownership, permission. If -pa is specified, then  \n",
      "                               preserves permission also because ACL is a        \n",
      "                               super-set of permission. Determination of whether \n",
      "                               raw namespace extended attributes are preserved is\n",
      "                               independent of the -p flag.                       \n",
      "  -f                           Overwrite the destination if it already exists.   \n",
      "  -d                           Skip creation of temporary file(<dst>._COPYING_). \n",
      "  -t <thread count>            Number of threads to be used, default is 1.       \n",
      "  -q <thread pool queue size>  Thread pool queue size to be used, default is     \n",
      "                               1024.                                             \n",
      "\n",
      "-createSnapshot <snapshotDir> [<snapshotName>] :\n",
      "  Create a snapshot on a directory\n",
      "\n",
      "-deleteSnapshot <snapshotDir> <snapshotName> :\n",
      "  Delete a snapshot from a directory\n",
      "\n",
      "-df [-h] [<path> ...] :\n",
      "  Shows the capacity, free and used space of the filesystem. If the filesystem has\n",
      "  multiple partitions, and no path to a particular partition is specified, then\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  the status of the root partitions will be shown.\n",
      "                                                                                 \n",
      "  -h  Formats the sizes of files in a human-readable fashion rather than a number\n",
      "      of bytes.                                                                  \n",
      "\n",
      "-du [-s] [-h] [-v] [-x] <path> ... :\n",
      "  Show the amount of space, in bytes, used by the files that match the specified\n",
      "  file pattern. The following flags are optional:\n",
      "                                                                                 \n",
      "  -s  Rather than showing the size of each individual file that matches the      \n",
      "      pattern, shows the total (summary) size.                                   \n",
      "  -h  Formats the sizes of files in a human-readable fashion rather than a number\n",
      "      of bytes.                                                                  \n",
      "  -v  option displays a header line.                                             \n",
      "  -x  Excludes snapshots from being counted.                                     \n",
      "  \n",
      "  Note that, even without the -s option, this only shows size summaries one level\n",
      "  deep into a directory.\n",
      "  \n",
      "  The output is in the form \n",
      "  \tsize\tdisk space consumed\tname(full path)\n",
      "\n",
      "-expunge [-immediate] [-fs <path>] :\n",
      "  Delete files from the trash that are older than the retention threshold\n",
      "\n",
      "-find <path> ... <expression> ... :\n",
      "  Finds all files that match the specified expression and\n",
      "  applies selected actions to them. If no <path> is specified\n",
      "  then defaults to the current working directory. If no\n",
      "  expression is specified then defaults to -print.\n",
      "  \n",
      "  The following primary expressions are recognised:\n",
      "    -name pattern\n",
      "    -iname pattern\n",
      "      Evaluates as true if the basename of the file matches the\n",
      "      pattern using standard file system globbing.\n",
      "      If -iname is used then the match is case insensitive.\n",
      "  \n",
      "    -print\n",
      "    -print0\n",
      "      Always evaluates to true. Causes the current pathname to be\n",
      "      written to standard output followed by a newline. If the -print0\n",
      "      expression is used then an ASCII NULL character is appended rather\n",
      "      than a newline.\n",
      "  \n",
      "  The following operators are recognised:\n",
      "    expression -a expression\n",
      "    expression -and expression\n",
      "    expression expression\n",
      "      Logical AND operator for joining two expressions. Returns\n",
      "      true if both child expressions return true. Implied by the\n",
      "      juxtaposition of two expressions and so does not need to be\n",
      "      explicitly specified. The second expression will not be\n",
      "      applied if the first fails.\n",
      "\n",
      "-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst> :\n",
      "  Copy files that match the file pattern <src> to the local name. <src> is kept.\n",
      "  When copying multiple files, the destination must be a directory.\n",
      "  Flags:\n",
      "                                                                                 \n",
      "  -p                           Preserves timestamps, ownership and the mode.     \n",
      "  -f                           Overwrites the destination if it already exists.  \n",
      "  -crc                         write CRC checksums for the files downloaded.     \n",
      "  -ignoreCrc                   Skip CRC checks on the file(s) downloaded.        \n",
      "  -t <thread count>            Number of threads to be used, default is 1.       \n",
      "  -q <thread pool queue size>  Thread pool queue size to be used, default is     \n",
      "                               1024.                                             \n",
      "\n",
      "-getfacl [-R] <path> :\n",
      "  Displays the Access Control Lists (ACLs) of files and directories. If a\n",
      "  directory has a default ACL, then getfacl also displays the default ACL.\n",
      "                                                                  \n",
      "  -R      List the ACLs of all files and directories recursively. \n",
      "  <path>  File or directory to list.                              \n",
      "\n",
      "-getfattr [-R] {-n name | -d} [-e en] <path> :\n",
      "  Displays the extended attribute names and values (if any) for a file or\n",
      "  directory.\n",
      "                                                                                 \n",
      "  -R             Recursively list the attributes for all files and directories.  \n",
      "  -n name        Dump the named extended attribute value.                        \n",
      "  -d             Dump all extended attribute values associated with pathname.    \n",
      "  -e <encoding>  Encode values after retrieving them.Valid encodings are \"text\", \n",
      "                 \"hex\", and \"base64\". Values encoded as text strings are enclosed\n",
      "                 in double quotes (\"), and values encoded as hexadecimal and     \n",
      "                 base64 are prefixed with 0x and 0s, respectively.               \n",
      "  <path>         The file or directory.                                          \n",
      "\n",
      "-getmerge [-nl] [-skip-empty-file] <src> <localdst> :\n",
      "  Get all the files in the directories that match the source file pattern and\n",
      "  merge and sort them to only one file on local fs. <src> is kept.\n",
      "                                                                     \n",
      "  -nl               Add a newline character at the end of each file. \n",
      "  -skip-empty-file  Do not add new line character for empty file.    \n",
      "\n",
      "-head <file> :\n",
      "  Show the first 1KB of the file.\n",
      "\n",
      "-help [cmd ...] :\n",
      "  Displays help for given command or all commands if none is specified.\n",
      "\n",
      "-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...] :\n",
      "  List the contents that match the specified file pattern. If path is not\n",
      "  specified, the contents of /user/<currentUser> will be listed. For a directory a\n",
      "  list of its direct children is returned (unless -d option is specified).\n",
      "  \n",
      "  Directory entries are of the form:\n",
      "  \tpermissions - userId groupId sizeOfDirectory(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) directoryName\n",
      "  \n",
      "  and file entries are of the form:\n",
      "  \tpermissions numberOfReplicas userId groupId sizeOfFile(in bytes)\n",
      "  modificationDate(yyyy-MM-dd HH:mm) fileName\n",
      "  \n",
      "    -C  Display the paths of files and directories only.\n",
      "    -d  Directories are listed as plain files.\n",
      "    -h  Formats the sizes of files in a human-readable fashion\n",
      "        rather than a number of bytes.\n",
      "    -q  Print ? instead of non-printable characters.\n",
      "    -R  Recursively list the contents of directories.\n",
      "    -t  Sort files by modification time (most recent first).\n",
      "    -S  Sort files by size.\n",
      "    -r  Reverse the order of the sort.\n",
      "    -u  Use time of last access instead of modification for\n",
      "        display and sorting.\n",
      "    -e  Display the erasure coding policy of files and directories.\n",
      "\n",
      "-mkdir [-p] <path> ... :\n",
      "  Create a directory in specified location.\n",
      "                                                  \n",
      "  -p  Do not fail if the directory already exists \n",
      "\n",
      "-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst> :\n",
      "  Same as -put, except that the source is deleted after it's copied\n",
      "  and -t option has not yet implemented.\n",
      "\n",
      "-moveToLocal <src> <localdst> :\n",
      "  Not implemented yet\n",
      "\n",
      "-mv <src> ... <dst> :\n",
      "  Move files that match the specified file pattern <src> to a destination <dst>. \n",
      "  When moving multiple files, the destination must be a directory.\n",
      "\n",
      "-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst> :\n",
      "  Copy files from the local file system into fs. Copying fails if the file already\n",
      "  exists, unless the -f flag is given.\n",
      "  Flags:\n",
      "                                                                                 \n",
      "  -p                           Preserves timestamps, ownership and the mode.     \n",
      "  -f                           Overwrites the destination if it already exists.  \n",
      "  -t <thread count>            Number of threads to be used, default is 1.       \n",
      "  -q <thread pool queue size>  Thread pool queue size to be used, default is     \n",
      "                               1024.                                             \n",
      "  -l                           Allow DataNode to lazily persist the file to disk.\n",
      "                               Forces replication factor of 1. This flag will    \n",
      "                               result in reduced durability. Use with care.      \n",
      "  -d                           Skip creation of temporary file(<dst>._COPYING_). \n",
      "\n",
      "-renameSnapshot <snapshotDir> <oldName> <newName> :\n",
      "  Rename a snapshot from oldName to newName\n",
      "\n",
      "-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Delete all files that match the specified file pattern. Equivalent to the Unix\n",
      "  command \"rm <src>\"\n",
      "                                                                                 \n",
      "  -f          If the file does not exist, do not display a diagnostic message or \n",
      "              modify the exit status to reflect an error.                        \n",
      "  -[rR]       Recursively deletes directories.                                   \n",
      "  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  \n",
      "  -safely     option requires safety confirmation, if enabled, requires          \n",
      "              confirmation before deleting large directory with more than        \n",
      "              <hadoop.shell.delete.limit.num.files> files. Delay is expected when\n",
      "              walking over large directory recursively to count the number of    \n",
      "              files to be deleted before the confirmation.                       \n",
      "\n",
      "-rmdir [--ignore-fail-on-non-empty] <dir> ... :\n",
      "  Removes the directory entry specified by each directory argument, provided it is\n",
      "  empty.\n",
      "\n",
      "-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>] :\n",
      "  Sets Access Control Lists (ACLs) of files and directories.\n",
      "  Options:\n",
      "                                                                                 \n",
      "  -b          Remove all but the base ACL entries. The entries for user, group   \n",
      "              and others are retained for compatibility with permission bits.    \n",
      "  -k          Remove the default ACL.                                            \n",
      "  -R          Apply operations to all files and directories recursively.         \n",
      "  -m          Modify ACL. New entries are added to the ACL, and existing entries \n",
      "              are retained.                                                      \n",
      "  -x          Remove specified ACL entries. Other ACL entries are retained.      \n",
      "  --set       Fully replace the ACL, discarding all existing entries. The        \n",
      "              <acl_spec> must include entries for user, group, and others for    \n",
      "              compatibility with permission bits. If the ACL spec contains only  \n",
      "              access entries, then the existing default entries are retained. If \n",
      "              the ACL spec contains only default entries, then the existing      \n",
      "              access entries are retained. If the ACL spec contains both access  \n",
      "              and default entries, then both are replaced.                       \n",
      "  <acl_spec>  Comma separated list of ACL entries.                               \n",
      "  <path>      File or directory to modify.                                       \n",
      "\n",
      "-setfattr {-n name [-v value] | -x name} <path> :\n",
      "  Sets an extended attribute name and value for a file or directory.\n",
      "                                                                                 \n",
      "  -n name   The extended attribute name.                                         \n",
      "  -v value  The extended attribute value. There are three different encoding     \n",
      "            methods for the value. If the argument is enclosed in double quotes, \n",
      "            then the value is the string inside the quotes. If the argument is   \n",
      "            prefixed with 0x or 0X, then it is taken as a hexadecimal number. If \n",
      "            the argument begins with 0s or 0S, then it is taken as a base64      \n",
      "            encoding.                                                            \n",
      "  -x name   Remove the extended attribute.                                       \n",
      "  <path>    The file or directory.                                               \n",
      "\n",
      "-setrep [-R] [-w] <rep> <path> ... :\n",
      "  Set the replication level of a file. If <path> is a directory then the command\n",
      "  recursively changes the replication factor of all files under the directory tree\n",
      "  rooted at <path>. The EC files will be ignored here.\n",
      "                                                                                 \n",
      "  -w  It requests that the command waits for the replication to complete. This   \n",
      "      can potentially take a very long time.                                     \n",
      "  -R  It is accepted for backwards compatibility. It has no effect.              \n",
      "\n",
      "-stat [format] <path> ... :\n",
      "  Print statistics about the file/directory at <path>\n",
      "  in the specified format. Format accepts permissions in\n",
      "  octal (%a) and symbolic (%A), filesize in\n",
      "  bytes (%b), type (%F), group name of owner (%g),\n",
      "  name (%n), block size (%o), replication (%r), user name\n",
      "  of owner (%u), access date (%x, %X).\n",
      "  modification date (%y, %Y).\n",
      "  %x and %y show UTC date as \"yyyy-MM-dd HH:mm:ss\" and\n",
      "  %X and %Y show milliseconds since January 1, 1970 UTC.\n",
      "  If the format is not specified, %y is used by default.\n",
      "\n",
      "-tail [-f] [-s <sleep interval>] <file> :\n",
      "  Show the last 1KB of the file.\n",
      "                                                                               \n",
      "  -f  Shows appended data as the file grows.                                   \n",
      "  -s  With -f , defines the sleep interval between iterations in milliseconds. \n",
      "\n",
      "-test -[defswrz] <path> :\n",
      "  Answer various questions about <path>, with result via exit status.\n",
      "    -d  return 0 if <path> is a directory.\n",
      "    -e  return 0 if <path> exists.\n",
      "    -f  return 0 if <path> is a file.\n",
      "    -s  return 0 if file <path> is greater         than zero bytes in size.\n",
      "    -w  return 0 if file <path> exists         and write permission is granted.\n",
      "    -r  return 0 if file <path> exists         and read permission is granted.\n",
      "    -z  return 0 if file <path> is         zero bytes in size, else return 1.\n",
      "\n",
      "-text [-ignoreCrc] <src> ... :\n",
      "  Takes a source file and outputs the file in text format.\n",
      "  The allowed formats are zip and TextRecordInputStream and Avro.\n",
      "\n",
      "-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] <path> ... :\n",
      "  Updates the access and modification times of the file specified by the <path> to\n",
      "  the current time. If the file does not exist, then a zero length file is created\n",
      "  at <path> with current time as the timestamp of that <path>.\n",
      "  -a Change only the access time \n",
      "  -m Change only the modification time \n",
      "  -t TIMESTAMP Use specified timestamp instead of current time\n",
      "   TIMESTAMP format yyyyMMdd:HHmmss\n",
      "  -c Do not create any files\n",
      "\n",
      "-touchz <path> ... :\n",
      "  Creates a file of zero length at <path> with current time as the timestamp of\n",
      "  that <path>. An error is returned if the file exists with non-zero length\n",
      "\n",
      "-truncate [-w] <length> <path> ... :\n",
      "  Truncate all files that match the specified file pattern to the specified\n",
      "  length.\n",
      "                                                                                 \n",
      "  -w  Requests that the command wait for block recovery to complete, if          \n",
      "      necessary.                                                                 \n",
      "\n",
      "-usage [cmd ...] :\n",
      "  Displays the usage for given command or all commands if none is specified.\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de05db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - hadoop supergroup          0 2023-03-22 20:30 /user/hadoop/.sparkStaging\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3462ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir /user/${USER}/test-22-03-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f34000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - hadoop supergroup          0 2023-03-22 20:30 /user/hadoop/.sparkStaging\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-22 20:48 /user/hadoop/test-22-03-23\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e929f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-mkdir [-p] <path> ... :\n",
      "  Create a directory in specified location.\n",
      "                                                  \n",
      "  -p  Do not fail if the directory already exists \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92042d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options] -mkdir [-p] <path> ...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -usage mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6425da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rmdir /user/hadoop/test-22-03-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffbd6ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - hadoop supergroup          0 2023-03-22 20:30 /user/hadoop/.sparkStaging\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/${USER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2743729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :\n",
      "  Delete all files that match the specified file pattern. Equivalent to the Unix\n",
      "  command \"rm <src>\"\n",
      "                                                                                 \n",
      "  -f          If the file does not exist, do not display a diagnostic message or \n",
      "              modify the exit status to reflect an error.                        \n",
      "  -[rR]       Recursively deletes directories.                                   \n",
      "  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  \n",
      "  -safely     option requires safety confirmation, if enabled, requires          \n",
      "              confirmation before deleting large directory with more than        \n",
      "              <hadoop.shell.delete.limit.num.files> files. Delay is expected when\n",
      "              walking over large directory recursively to count the number of    \n",
      "              files to be deleted before the confirmation.                       \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -help rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11491790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hadoop/test-22-03-23\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -rm -R /user/hadoop/test-22-03-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbcd2c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst> :\n",
      "  Copy files from the local file system into fs. Copying fails if the file already\n",
      "  exists, unless the -f flag is given.\n",
      "  Flags:\n",
      "                                                                                 \n",
      "  -p                           Preserves timestamps, ownership and the mode.     \n",
      "  -f                           Overwrites the destination if it already exists.  \n",
      "  -t <thread count>            Number of threads to be used, default is 1.       \n",
      "  -q <thread pool queue size>  Thread pool queue size to be used, default is     \n",
      "                               1024.                                             \n",
      "  -l                           Allow DataNode to lazily persist the file to disk.\n",
      "                               Forces replication factor of 1. This flag will    \n",
      "                               result in reduced durability. Use with care.      \n",
      "  -d                           Skip creation of temporary file(<dst>._COPYING_). \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -help put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58a1ce41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst> :\n",
      "  Identical to the -put command.\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "hdfs dfs -help copyFromLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48493d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst> :\n",
      "  Copy files that match the file pattern <src> to the local name. <src> is kept.\n",
      "  When copying multiple files, the destination must be a directory.\n",
      "  Flags:\n",
      "                                                                                 \n",
      "  -p                           Preserves timestamps, ownership and the mode.     \n",
      "  -f                           Overwrites the destination if it already exists.  \n",
      "  -crc                         write CRC checksums for the files downloaded.     \n",
      "  -ignoreCrc                   Skip CRC checks on the file(s) downloaded.        \n",
      "  -t <thread count>            Number of threads to be used, default is 1.       \n",
      "  -q <thread pool queue size>  Thread pool queue size to be used, default is     \n",
      "                               1024.                                             \n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26fbb56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs fsck <path> [-list-corruptfileblocks | [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks | -replicaDetails | -upgradedomains]]]] [-includeSnapshots] [-showprogress] [-storagepolicies] [-maintenance] [-blockId <blk_Id>] [-replicate]\n",
      "\t<path>\tstart checking from this path\n",
      "\t-move\tmove corrupted files to /lost+found\n",
      "\t-delete\tdelete corrupted files\n",
      "\t-files\tprint out files being checked\n",
      "\t-openforwrite\tprint out files opened for write\n",
      "\t-includeSnapshots\tinclude snapshot data if the given path indicates a snapshottable directory or there are snapshottable directories under it\n",
      "\t-list-corruptfileblocks\tprint out list of missing blocks and files they belong to\n",
      "\t-files -blocks\tprint out block report\n",
      "\t-files -blocks -locations\tprint out locations for every block\n",
      "\t-files -blocks -racks\tprint out network topology for data-node locations\n",
      "\t-files -blocks -replicaDetails\tprint out each replica details \n",
      "\t-files -blocks -upgradedomains\tprint out upgrade domains for every block\n",
      "\t-storagepolicies\tprint out storage policy summary for the blocks\n",
      "\t-maintenance\tprint out maintenance state node details\n",
      "\t-showprogress\tDeprecated. Progress is now shown by default\n",
      "\t-blockId\tprint out which file this blockId belongs to, locations (nodes, racks) of this block, and other diagnostics info (under replicated, corrupted or not, etc)\n",
      "\t-replicate initiate replication work to make mis-replicated\n",
      " blocks satisfy block placement policy\n",
      "\n",
      "Please Note:\n",
      "\n",
      "\t1. By default fsck ignores files opened for write, use -openforwrite to report such files. They are usually  tagged CORRUPT or HEALTHY depending on their block allocation status\n",
      "\t2. Option -includeSnapshots should not be used for comparing stats, should be used only for HEALTH check, as this may contain duplicates if the same file present in both original fs tree and inside snapshots.\n",
      "\n",
      "Generic options supported are:\n",
      "-conf <configuration file>        specify an application configuration file\n",
      "-D <property=value>               define a value for a given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
      "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
      "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
      "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
      "\n",
      "The general command line syntax is:\n",
      "command [genericOptions] [commandOptions]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs fsck -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff342b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://master:9870/fsck?ugi=hadoop&path=%2Fuser%2Fhadoop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSCK started by hadoop (auth:SIMPLE) from /192.168.56.50 for path /user/hadoop at Wed Mar 22 21:12:07 UTC 2023\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t2\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t6\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t849583721 B\n",
      " Total files:\t7\n",
      " Total blocks (validated):\t12 (avg. block size 70798643 B)\n",
      " Minimally replicated blocks:\t12 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Wed Mar 22 21:12:07 UTC 2023 in 36 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs fsck /user/${USER}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3db8850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://master:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2F.sparkStaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSCK started by hadoop (auth:SIMPLE) from /192.168.56.50 for path /user/hadoop/.sparkStaging at Wed Mar 22 21:15:39 UTC 2023\n",
      "\n",
      "/user/hadoop/.sparkStaging <dir>\n",
      "/user/hadoop/.sparkStaging/application_1677261994101_0002 <dir>\n",
      "/user/hadoop/.sparkStaging/application_1677261994101_0002/__spark_conf__.zip 280516 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1567849437-192.168.56.50-1676055294185:blk_1073742410_1600 len=280516 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1677261994101_0002/__spark_libs__297832934108423430.zip 302748989 bytes, replicated: replication=2, 3 block(s):  OK\n",
      "0. BP-1567849437-192.168.56.50-1676055294185:blk_1073742407_1597 len=134217728 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "1. BP-1567849437-192.168.56.50-1676055294185:blk_1073742408_1598 len=134217728 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "2. BP-1567849437-192.168.56.50-1676055294185:blk_1073742409_1599 len=34313533 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1678980392466_0002 <dir>\n",
      "/user/hadoop/.sparkStaging/application_1679333213294_0001 <dir>\n",
      "/user/hadoop/.sparkStaging/application_1679333213294_0001/__spark_libs__3612270852538640750.zip 241958912 bytes, replicated: replication=2, 2 block(s):  OK\n",
      "0. BP-1567849437-192.168.56.50-1676055294185:blk_1073742573_1784 len=134217728 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "1. BP-1567849437-192.168.56.50-1676055294185:blk_1073742574_1785 len=107741184 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1679515925232_0003 <dir>\n",
      "/user/hadoop/.sparkStaging/application_1679515925232_0003/__spark_conf__.zip 273335 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1567849437-192.168.56.50-1676055294185:blk_1073742630_1842 len=273335 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1679515925232_0003/__spark_libs__2181048667937122115.zip 302748989 bytes, replicated: replication=2, 3 block(s):  OK\n",
      "0. BP-1567849437-192.168.56.50-1676055294185:blk_1073742625_1837 len=134217728 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "1. BP-1567849437-192.168.56.50-1676055294185:blk_1073742626_1838 len=134217728 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "2. BP-1567849437-192.168.56.50-1676055294185:blk_1073742627_1839 len=34313533 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK], DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK]]\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1679515925232_0003/py4j-0.10.9.5-src.zip 42404 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1567849437-192.168.56.50-1676055294185:blk_1073742629_1841 len=42404 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK], DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK]]\n",
      "\n",
      "/user/hadoop/.sparkStaging/application_1679515925232_0003/pyspark.zip 1530576 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1567849437-192.168.56.50-1676055294185:blk_1073742628_1840 len=1530576 Live_repl=2  [DatanodeInfoWithStorage[192.168.56.52:9866,DS-62c3702b-fb5e-40f3-9996-63012ed775c4,DISK], DatanodeInfoWithStorage[192.168.56.51:9866,DS-727f03c3-d496-440e-a7c3-5397a8a7e7fb,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t2\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t5\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t849583721 B\n",
      " Total files:\t7\n",
      " Total blocks (validated):\t12 (avg. block size 70798643 B)\n",
      " Minimally replicated blocks:\t12 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Wed Mar 22 21:15:39 UTC 2023 in 25 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/.sparkStaging' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs fsck /user/${USER}/.sparkStaging -files -blocks -locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bac5208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem            Size   Used  Available  Use%\n",
      "hdfs://master:9000  44.9 G  3.3 G     21.6 G    7%\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4aad069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-stat [format] <path> ... :\n",
      "  Print statistics about the file/directory at <path>\n",
      "  in the specified format. Format accepts permissions in\n",
      "  octal (%a) and symbolic (%A), filesize in\n",
      "  bytes (%b), type (%F), group name of owner (%g),\n",
      "  name (%n), block size (%o), replication (%r), user name\n",
      "  of owner (%u), access date (%x, %X).\n",
      "  modification date (%y, %Y).\n",
      "  %x and %y show UTC date as \"yyyy-MM-dd HH:mm:ss\" and\n",
      "  %X and %Y show milliseconds since January 1, 1970 UTC.\n",
      "  If the format is not specified, %y is used by default.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hdfs dfs -help stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bced0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
