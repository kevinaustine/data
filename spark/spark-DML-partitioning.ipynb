{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02054a65-0e36-4080-b4ee-850e57aa3192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1776df1e-ec50-4fe7-8b71-a3e2bb08e510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    enableHiveSupport().\\\n",
    "    appName(\"Spark SQL - Managing Tables - DML and Partitioning\").\\\n",
    "    master(\"yarn\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42b891d-459d-4b57-81cb-f8f46eaea6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|    exercise|\n",
      "|          hr|\n",
      "|       kevin|\n",
      "|kevin_retail|\n",
      "|      retail|\n",
      "|         sms|\n",
      "|        test|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acbb202-43d4-4303-bcbf-0113d2409087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use kevin_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0654b00-2917-471b-a377-a112ae037cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a394a379-5177-4653-a23d-0f92715984ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5881730c-6d6a-4dfd-9151-bc1343196fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE order_items (\n",
    "  order_item_id INT,\n",
    "  order_item_order_id INT,\n",
    "  order_item_product_id INT,\n",
    "  order_item_quantity INT,\n",
    "  order_item_subtotal FLOAT,\n",
    "  order_item_product_price FLOAT\n",
    ") STORED AS parquet\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bec82c2-c041-45bd-afe2-cf2c085821dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                         |comment|\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|order_item_id               |int                                                               |null   |\n",
      "|order_item_order_id         |int                                                               |null   |\n",
      "|order_item_product_id       |int                                                               |null   |\n",
      "|order_item_quantity         |int                                                               |null   |\n",
      "|order_item_subtotal         |float                                                             |null   |\n",
      "|order_item_product_price    |float                                                             |null   |\n",
      "|                            |                                                                  |       |\n",
      "|# Detailed Table Information|                                                                  |       |\n",
      "|Database                    |kevin_retail                                                      |       |\n",
      "|Table                       |order_items                                                       |       |\n",
      "|Owner                       |hadoop                                                            |       |\n",
      "|Created Time                |Mon Mar 27 19:47:34 UTC 2023                                      |       |\n",
      "|Last Access                 |UNKNOWN                                                           |       |\n",
      "|Created By                  |Spark 3.3.1                                                       |       |\n",
      "|Type                        |MANAGED                                                           |       |\n",
      "|Provider                    |hive                                                              |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1679946454]                                |       |\n",
      "|Location                    |hdfs://master:9000/user/hive/warehouse/kevin_retail.db/order_items|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe       |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat     |       |\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED order_items\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc666a-613f-4599-aed5-49d74e2cba71",
   "metadata": {},
   "source": [
    " ## LOAD vs. INSERT\n",
    " \n",
    "LOAD will copy the files by dividing them into blocks.\n",
    "LOAD is the fastest way of getting data into Spark Metastore tables. However, there will be minimal validations at File level. There will be no transformations or validations at data level.\n",
    "\n",
    "If it require any transformation while getting data into Spark Metastore table, then we need to use INSERT command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1018bb66-5f16-47d7-8c12-3b24a2087255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data from local files and not HDFS\n",
    "# Above load command will be successful, however when we try to query it will fail as the query expects data\n",
    "#to be in Parquet file format.\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/data/retail_db/order_items/' INTO TABLE order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46eb3554-261b-4a04-a6cf-cf80e7c4ad8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o64.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 4) (slave2 executor 2): java.lang.RuntimeException: hdfs://master:9000/user/hive/warehouse/kevin_retail.db/order_items/part-00000 is not a Parquet file. Expected magic number at tail, but found [48, 46, 48, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:269)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:268)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:554)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.RuntimeException: hdfs://master:9000/user/hive/warehouse/kevin_retail.db/order_items/part-00000 is not a Parquet file. Expected magic number at tail, but found [48, 46, 48, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:269)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:268)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:554)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM order_items LIMIT 10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/de-venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/de-venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/de-venv/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/de-venv/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o64.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 4) (slave2 executor 2): java.lang.RuntimeException: hdfs://master:9000/user/hive/warehouse/kevin_retail.db/order_items/part-00000 is not a Parquet file. Expected magic number at tail, but found [48, 46, 48, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:269)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:268)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:554)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.RuntimeException: hdfs://master:9000/user/hive/warehouse/kevin_retail.db/order_items/part-00000 is not a Parquet file. Expected magic number at tail, but found [48, 46, 48, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$lzycompute$1(ParquetFileFormat.scala:269)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.footerFileMetaData$1(ParquetFileFormat.scala:268)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$1(ParquetFileFormat.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:209)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:554)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM order_items LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f99b5-19e7-443e-8fe7-d7b420bc79d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aed8b4ac-3807-4d11-94f0-b035de6e7d0c",
   "metadata": {},
   "source": [
    "### Inserting Data using Stage Table\n",
    "\n",
    "As data is in text file format and our table is created with Parquet file format, we will not be able to use LOAD command to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa770d37-0a5f-4e0a-bada-569e90cff9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"TRUNCATE TABLE order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ca28865-f0ad-42d0-836d-4972303dd907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+-----------+\n",
      "|   namespace|        tableName|isTemporary|\n",
      "+------------+-----------------+-----------+\n",
      "|kevin_retail|      order_items|      false|\n",
      "|kevin_retail|order_items_stage|      false|\n",
      "|kevin_retail|           orders|      false|\n",
      "|kevin_retail|      orders_part|      false|\n",
      "+------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe4d66de-64f3-47ee-8573-b5d16735e2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS order_items_stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29ea6457-5d4b-43e2-a8a9-57574156ac26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE order_items_stage (\n",
    "  order_item_id INT,\n",
    "  order_item_order_id INT,\n",
    "  order_item_product_id INT,\n",
    "  order_item_quantity INT,\n",
    "  order_item_subtotal FLOAT,\n",
    "  order_item_product_price FLOAT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8827235-79a9-4f5b-8e95-58657ec87eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                               |comment|\n",
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "|order_item_id               |int                                                                     |null   |\n",
      "|order_item_order_id         |int                                                                     |null   |\n",
      "|order_item_product_id       |int                                                                     |null   |\n",
      "|order_item_quantity         |int                                                                     |null   |\n",
      "|order_item_subtotal         |float                                                                   |null   |\n",
      "|order_item_product_price    |float                                                                   |null   |\n",
      "|                            |                                                                        |       |\n",
      "|# Detailed Table Information|                                                                        |       |\n",
      "|Database                    |kevin_retail                                                            |       |\n",
      "|Table                       |order_items_stage                                                       |       |\n",
      "|Owner                       |hadoop                                                                  |       |\n",
      "|Created Time                |Mon Mar 27 19:49:09 UTC 2023                                            |       |\n",
      "|Last Access                 |UNKNOWN                                                                 |       |\n",
      "|Created By                  |Spark 3.3.1                                                             |       |\n",
      "|Type                        |MANAGED                                                                 |       |\n",
      "|Provider                    |hive                                                                    |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1679946549]                                      |       |\n",
      "|Location                    |hdfs://master:9000/user/hive/warehouse/kevin_retail.db/order_items_stage|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                      |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat              |       |\n",
      "|Storage Properties          |[serialization.format=,, field.delim=,]                                 |       |\n",
      "|Partition Provider          |Catalog                                                                 |       |\n",
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED order_items_stage\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6b7f4c2-19e6-4c06-ae4a-468dcd296d92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/data/retail_db/order_items/'  INTO TABLE order_items_stage \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b903bd0f-a14a-4f18-abcf-4b6c388b3d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM order_items_stage LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdff9670-d4b4-400c-bbea-aa40529f58be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO TABLE order_items SELECT * FROM order_items_stage limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25f17cb7-597b-4f71-a71e-e463f1bdfe00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|        87943|              35210|                 1014|                  4|             199.92|                   49.98|\n",
      "|        87944|              35211|                  365|                  4|             239.96|                   59.99|\n",
      "|        87945|              35212|                 1014|                  1|              49.98|                   49.98|\n",
      "|        87946|              35212|                  191|                  3|             299.97|                   99.99|\n",
      "|        87947|              35212|                 1014|                  5|              249.9|                   49.98|\n",
      "|        87948|              35212|                 1014|                  1|              49.98|                   49.98|\n",
      "|        87949|              35212|                 1014|                  3|             149.94|                   49.98|\n",
      "|        87950|              35213|                  365|                  4|             239.96|                   59.99|\n",
      "|        87951|              35213|                  502|                  3|              150.0|                    50.0|\n",
      "|        87952|              35213|                  957|                  1|             299.98|                  299.98|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM order_items_stage limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1590a27b-a4d8-4f6a-a348-5dc3ad840500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  172198|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(1) FROM order_items\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7982452-0867-4a1f-a206-a9457ab9321f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#INSERT OVERWRITE will overwrite the data in target table by deleting the files related to old data from the \n",
    "#directory pointed by the Spark Metastore table.\n",
    "\n",
    "spark.sql(\"\"\"INSERT OVERWRITE TABLE order_items\n",
    "SELECT * FROM order_items_stage limit \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b683525-84f4-4d6a-a834-953bd7495b69",
   "metadata": {},
   "source": [
    "### Creating Partitioned Tables\n",
    "\n",
    "We can use PARTITIONED BY clause to define the column along with data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a640bd7-cf1e-4433-a8ff-49960866ab32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS orders_part (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") PARTITIONED BY (order_month INT)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86e84297-effa-4f44-8291-994a3017f289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                         |comment|\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                               |null   |\n",
      "|order_date                  |string                                                            |null   |\n",
      "|order_customer_id           |int                                                               |null   |\n",
      "|order_status                |string                                                            |null   |\n",
      "|order_month                 |string                                                            |null   |\n",
      "|# Partition Information     |                                                                  |       |\n",
      "|# col_name                  |data_type                                                         |comment|\n",
      "|order_month                 |string                                                            |null   |\n",
      "|                            |                                                                  |       |\n",
      "|# Detailed Table Information|                                                                  |       |\n",
      "|Database                    |kevin_retail                                                      |       |\n",
      "|Table                       |orders_part                                                       |       |\n",
      "|Owner                       |hadoop                                                            |       |\n",
      "|Created Time                |Mon Mar 27 17:01:00 UTC 2023                                      |       |\n",
      "|Last Access                 |UNKNOWN                                                           |       |\n",
      "|Created By                  |Spark 3.3.1                                                       |       |\n",
      "|Type                        |MANAGED                                                           |       |\n",
      "|Provider                    |hive                                                              |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1679936460]                                |       |\n",
      "|Location                    |hdfs://master:9000/user/hive/warehouse/kevin_retail.db/orders_part|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                          |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat        |       |\n",
      "|Storage Properties          |[serialization.format=,, field.delim=,]                           |       |\n",
      "|Partition Provider          |Catalog                                                           |       |\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED orders_part\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b94d4f95-c00d-47a0-910f-dfa7fac5ec74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 items\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:01 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201307\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:02 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201308\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:02 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201309\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:02 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201310\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:03 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201311\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201312\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201401\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201402\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201403\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201404\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201405\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201406\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 17:05 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201407\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/kevin_retail.db/orders_part/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b1ef0-9b12-4fc2-9401-5df6afe702bc",
   "metadata": {},
   "source": [
    "### Adding Partitions to Tables\n",
    "\n",
    "We can add partitions using ALTER TABLE command with ADD PARTITION.\n",
    "For each and every partition created, a subdirectory will be created using partition column name and corresponding value under the table directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "859ce288-f032-427c-8458-1c085cebf59c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS orders_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b18898f7-93bb-4016-b82e-f859db67a1de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS orders_part (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") PARTITIONED BY (order_month STRING)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cd550f2-5e62-4a9b-8f65-73595319a4f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/kevin_retail.db/orders_part/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c018980-2d5b-4ae1-81b5-c2cc8856626c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                         |comment|\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                               |null   |\n",
      "|order_date                  |string                                                            |null   |\n",
      "|order_customer_id           |int                                                               |null   |\n",
      "|order_status                |string                                                            |null   |\n",
      "|order_month                 |string                                                            |null   |\n",
      "|# Partition Information     |                                                                  |       |\n",
      "|# col_name                  |data_type                                                         |comment|\n",
      "|order_month                 |string                                                            |null   |\n",
      "|                            |                                                                  |       |\n",
      "|# Detailed Table Information|                                                                  |       |\n",
      "|Database                    |kevin_retail                                                      |       |\n",
      "|Table                       |orders_part                                                       |       |\n",
      "|Owner                       |hadoop                                                            |       |\n",
      "|Created Time                |Mon Mar 27 19:50:13 UTC 2023                                      |       |\n",
      "|Last Access                 |UNKNOWN                                                           |       |\n",
      "|Created By                  |Spark 3.3.1                                                       |       |\n",
      "|Type                        |MANAGED                                                           |       |\n",
      "|Provider                    |hive                                                              |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1679946613]                                |       |\n",
      "|Location                    |hdfs://master:9000/user/hive/warehouse/kevin_retail.db/orders_part|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                          |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat        |       |\n",
      "|Storage Properties          |[serialization.format=,, field.delim=,]                           |       |\n",
      "|Partition Provider          |Catalog                                                           |       |\n",
      "+----------------------------+------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED orders_part\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ac46606-890f-4e71-ad94-924822c79c11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE orders_part ADD PARTITION (order_month=201307)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "710b93bd-40dc-4467-8bdb-20514ee01ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"ALTER TABLE orders_part ADD\n",
    "    PARTITION (order_month=201308)\n",
    "    PARTITION (order_month=201309)\n",
    "    PARTITION (order_month=201310)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ee78211-50a8-4fc5-b9a8-43c56e1fd370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201307\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201308\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201309\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201310\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/kevin_retail.db/orders_part/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29c5d2-5780-4b36-8596-47bcab7c3f4c",
   "metadata": {},
   "source": [
    "### Loading into Partitions\n",
    "\n",
    "- We need to make sure that file format of the file which is being loaded into table is same as the file format used while creating the table.\n",
    "- We also need to make sure that delimiters are consistent between files and table for text file format.\n",
    "- Also data should match the criteria for the partition into which data is loaded.\n",
    "- Our */home/hadoop/data/data/retail_db/orders* have data for the whole year and hence we should not load the data directly into partition.\n",
    "- We need to split into files matching partition criteria and then load into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b932311-f5a8-4667-8242-58fd34ce4114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/data/data/partition/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d6221f3-6e14-4720-b4b9-874335e77b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!grep 2013-07 /home/hadoop/data/data/retail_db/orders/part-00000 > /home/hadoop/data/data/partition/orders/orders_201307\n",
    "!grep 2013-08 /home/hadoop/data/data/retail_db/orders/part-00000 > /home/hadoop/data/data/partition/orders/orders_201308\n",
    "!grep 2013-09 /home/hadoop/data/data/retail_db/orders/part-00000 > /home/hadoop/data/data/partition/orders/orders_201309\n",
    "!grep 2013-10 /home/hadoop/data/data/retail_db/orders/part-00000 > /home/hadoop/data/data/partition/orders/orders_201310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af332ecf-c8e7-4ca1-a4c5-c25b515f17b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/data/partition/orders/orders_201307'INTO TABLE orders_part PARTITION (order_month=201307)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cc34c92-0e57-4f03-be70-fb3e6757998d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/data/partition/orders/orders_201308' INTO TABLE orders_part PARTITION (order_month=201308)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbf65adf-fcc1-4907-9b18-1a956a52cf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/data/partition/orders/orders_201309' INTO TABLE orders_part PARTITION (order_month=201309)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f7012da-67ae-484c-bcce-9b363e9efbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/data/partition/orders/orders_201310' INTO TABLE orders_part PARTITION (order_month=201310)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9fb789-8378-4220-82f5-249312750d77",
   "metadata": {},
   "source": [
    "### Inserting Data into Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33bd0592-2223-4079-9689-e27eeae37c85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE orders_part ADD PARTITION (order_month=201311)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89530e6f-3a79-4ec3-83d7-49f97a731b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"INSERT INTO TABLE orders_part PARTITION (order_month=201311)\n",
    "  SELECT * FROM orders WHERE order_date LIKE '2013-11%'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a299f090-ec0b-427b-8e8c-c8f5aa22efbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   24770|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(1) FROM orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbd0070f-3508-4cd0-a6da-b7c27a1dc69e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201307\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201308\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201309\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201310\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201311\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/kevin_retail.db/orders_part/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64421890-1528-4bbe-9273-90e6647ef3dd",
   "metadata": {},
   "source": [
    "## Using Dynamic Partition Mode\n",
    "\n",
    "- Using dynamic partition mode we need not pre create the partitions. Partitions will be automatically created when we issue INSERT command in dynamic - partition mode.\n",
    "- To insert data using dynamic partition mode, we need to set the property hive.exec.dynamic.partition to true.\n",
    "- Also we need to set hive.exec.dynamic.partition.mode to nonstrict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ba10d39-fbb7-4910-8e7d-e8e2c4ccae13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                 key|      value|\n",
      "+--------------------+-----------+\n",
      "|hive.exec.dynamic...|<undefined>|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33e71ae5-a61b-4807-9865-b13a0aec3a58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                 key|      value|\n",
      "+--------------------+-----------+\n",
      "|hive.exec.dynamic...|<undefined>|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition.mode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6cfb1a6-3a71-4577-a6d1-80bba74633fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee2b195a-ea68-496b-94b3-709691266b09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition.mode=nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "055a2701-fe47-474c-a2f3-6b6efe694505",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 key|value|\n",
      "+--------------------+-----+\n",
      "|hive.exec.dynamic...| true|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01e75f61-4f71-4bfc-b5b4-df414c7a4c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                 key|    value|\n",
      "+--------------------+---------+\n",
      "|hive.exec.dynamic...|nonstrict|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition.mode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b95a1fea-6f38-4d05-929f-567b5ae9b818",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"INSERT INTO TABLE orders_part PARTITION (order_month)\n",
    "SELECT o.*, date_format(order_date, 'yyyyMM') order_month\n",
    "FROM orders o\n",
    "WHERE order_date >= '2013-12-01 00:00:00.0'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "807ce3b5-f4a7-4c7f-9eda-9765609b9abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 items\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201307\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201308\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201309\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201310\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:50 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201311\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201312\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201401\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201402\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201403\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201404\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201405\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201406\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-27 19:51 /user/hive/warehouse/kevin_retail.db/orders_part/order_month=201407\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/kevin_retail.db/orders_part/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20bbcc65-0b64-43d1-8908-c679a8a220b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         partition|\n",
      "+------------------+\n",
      "|order_month=201307|\n",
      "|order_month=201308|\n",
      "|order_month=201309|\n",
      "|order_month=201310|\n",
      "|order_month=201311|\n",
      "|order_month=201312|\n",
      "|order_month=201401|\n",
      "|order_month=201402|\n",
      "|order_month=201403|\n",
      "|order_month=201404|\n",
      "|order_month=201405|\n",
      "|order_month=201406|\n",
      "|order_month=201407|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show partitions kevin_retail.orders_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e9212-e85e-4f95-82c5-b2f443ee04a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
