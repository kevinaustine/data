{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34685371-b248-489d-b014-9798bf1110e6",
   "metadata": {},
   "source": [
    "- Use data from /data/nyse_all/nyse_data\n",
    "\n",
    "- Use database YOUR_OS_USER_NAME_nyse\n",
    "\n",
    "- Create partitioned table nyse_eod_part\n",
    "\n",
    "- Field Names: stockticker, tradedate, openprice, highprice, lowprice, closeprice, volume\n",
    "\n",
    "- Determine correct data types based on the values\n",
    "\n",
    "- Create Managed table with “,” as delimiter.\n",
    "\n",
    "- Partition Field should be tradeyear and of type INT (one partition for corresponding year)\n",
    "\n",
    "- Insert data into partitioned table using dynamic partition mode.\n",
    "\n",
    "- Here are the steps to come up with the solution.\n",
    "\n",
    ">Review the files under /data/nyse_all/nyse_data - determine data types (For example: tradedate should be INT and volume should be BIGINT)\n",
    "\n",
    ">Create database YOUR_OS_USER_NAME_nyse (if it does not exists)\n",
    "\n",
    ">Create non partitioned stage table\n",
    "\n",
    ">Load data into non partitioned stage table\n",
    "\n",
    ">Validate the count and also see that data is as expected by running simple select query.\n",
    "\n",
    ">Create partitioned table\n",
    "\n",
    ">Set required properties to use dynamic partition\n",
    "\n",
    ">Insert data into partitioned table - here is how you can compute year from tradedate of type int year(to_date(cast(tradedate AS STRING), 'yyyyMMdd')) AS tradeyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17699cde-bac5-41b7-9fed-28ec36ac22be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df85de75-7370-4bb7-8923-9c14c3a31928",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    enableHiveSupport().\\\n",
    "    appName(\"Spark SQL -Exercise Partitioning\").\\\n",
    "    master(\"yarn\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f6c79c-213f-49ae-8c9d-3fc19c8f3e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                 key|      value|\n",
      "+--------------------+-----------+\n",
      "|hive.exec.dynamic...|<undefined>|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06575a5-e9b0-472d-9b45-850d3f2b2e54",
   "metadata": {},
   "source": [
    "### Set required properties to use dynamic partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba0674f-b57f-4550-8300-1b97cfcc7263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                 key|      value|\n",
      "+--------------------+-----------+\n",
      "|hive.exec.dynamic...|<undefined>|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition.mode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a76619d-3cc2-47ac-a2d9-5b4c4081969e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336a3aa8-9b23-43dc-a135-f775ce0763b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists exercise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46557c16-766f-4acf-a921-a9e7d20f9af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition.mode=nonstrict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17add6e6-ab58-4ebe-b9a6-57593c394d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|    exercise|\n",
      "|          hr|\n",
      "|       kevin|\n",
      "|kevin_retail|\n",
      "|      retail|\n",
      "|         sms|\n",
      "|        test|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c64c92-7546-4bb7-acd9-a45c8b738564",
   "metadata": {},
   "source": [
    "### Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce435e2-24b4-4ab8-b4bc-d673337abac2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733a625-de57-4242-96d6-586b5d3c1779",
   "metadata": {},
   "source": [
    "### Select the database to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63c19b9c-0edb-478f-a736-f78702d32bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|          exercise|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select current_database()\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71839e94-5ac5-4592-ad67-5d00eefd9a12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Listing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2e0214-5227-440a-b891-9e31258f24e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYSE_1997.txt.gz  NYSE_2003.txt.gz  NYSE_2009.txt.gz  NYSE_2015.txt.gz\n",
      "NYSE_1998.txt.gz  NYSE_2004.txt.gz  NYSE_2010.txt.gz  NYSE_2016.txt.gz\n",
      "NYSE_1999.txt.gz  NYSE_2005.txt.gz  NYSE_2011.txt.gz  NYSE_2017.txt.gz\n",
      "NYSE_2000.txt.gz  NYSE_2006.txt.gz  NYSE_2012.txt.gz\n",
      "NYSE_2001.txt.gz  NYSE_2007.txt.gz  NYSE_2013.txt.gz\n",
      "NYSE_2002.txt.gz  NYSE_2008.txt.gz  NYSE_2014.txt.gz\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "! ls /home/hadoop/data/data/nyse_all/nyse_data/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360aab8-e4f0-4a77-91b4-5c5ff60758e6",
   "metadata": {},
   "source": [
    "### Review and determine data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8f5c7-f3cf-4e9c-884e-e1e777804f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA,19970101,47.82,47.82,47.82,47.82,0\n",
      "ABC,19970101,6.03,6.03,6.03,6.03,0\n",
      "ABM,19970101,9.25,9.25,9.25,9.25,0\n",
      "ABT,19970101,25.37,25.37,25.37,25.37,0\n",
      "ABX,19970101,28.75,28.75,28.75,28.75,0\n",
      "ACP,19970101,9.12,9.12,9.12,9.12,0\n",
      "ACV,19970101,16,16,16,16,0\n",
      "ADC,19970101,21.37,21.37,21.37,21.37,0\n",
      "ADM,19970101,17.24,17.24,17.24,17.24,0\n",
      "ADX,19970101,13.16,13.16,13.16,13.16,0\n",
      "AED,19970101,31.5,31.5,31.5,31.5,0\n",
      "AEE,19970101,38.5,38.5,38.5,38.5,0\n",
      "AEG,19970101,15.2,15.2,15.2,15.2,0\n",
      "AEM,19970101,14,14,14,14,0\n",
      "AEP,19970101,41.12,41.12,41.12,41.12,0\n",
      "AES,19970101,11.62,11.62,11.62,11.62,0\n",
      "AF,19970101,12.29,12.29,12.29,12.29,0\n",
      "AFG,19970101,25.179,25.179,25.179,25.179,0\n",
      "AFL,19970101,10.69,10.69,10.69,10.69,0\n",
      "AG,19970101,28.62,28.62,28.62,28.62,0\n",
      "AGCO,19970101,28.625,28.625,28.625,28.625,0\n",
      "AGM,19970101,10.25,10.25,10.25,10.25,0\n",
      "AGM.A,19970101,26.5,26.5,26.5,26.5,0\n",
      "\u001b[7m/home/hadoop/data/data/nyse_all/nyse_data/NYSE_1997.txt.gz\u001b[m\u001b[K"
     ]
    }
   ],
   "source": [
    "!zless /home/hadoop/data/data/nyse_all/nyse_data/NYSE_1997.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307200d-ff18-4fe7-bcc2-baf6094234cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create non partitioned stage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e02795f-ef40-4732-9efa-712dec04d54b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS nyse_staging (\n",
    "  stockticker STRING,\n",
    "  tradedate STRING,\n",
    "  openprice FLOAT,\n",
    "  highprice FLOAT,\n",
    "  lowprice FLOAT,\n",
    "  closeprice FLOAT,\n",
    "  volume INT\n",
    ") \n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba88883-201a-4e11-8c1d-ce6b0e0cf317",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading data into non partitioned stage table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c665b27e-e37d-4976-a0b7-13ec551800ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/data/nyse_all/nyse_data/*'  INTO TABLE nyse_staging \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123f51b-cbcb-4914-bc0a-5bad02af02fb",
   "metadata": {},
   "source": [
    "### Validating the count and also see that data is as expected by running select query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "005fbd33-7228-48aa-abb2-11d5e21dcc61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================================>             (16 + 2) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+---------+--------+----------+------+\n",
      "|stockticker|tradedate|openprice|highprice|lowprice|closeprice|volume|\n",
      "+-----------+---------+---------+---------+--------+----------+------+\n",
      "|AA         |19980101 |52.77    |52.77    |52.77   |52.77     |0     |\n",
      "|ABC        |19980101 |7.28     |7.28     |7.28    |7.28      |0     |\n",
      "|ABM        |19980101 |15.28    |15.28    |15.28   |15.28     |0     |\n",
      "|ABT        |19980101 |32.75    |32.75    |32.75   |32.75     |0     |\n",
      "|ABX        |19980101 |18.62    |18.62    |18.62   |18.62     |0     |\n",
      "+-----------+---------+---------+---------+--------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nyse_staging LIMIT 10\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec76e0-e631-4ad9-bf44-1891b392967b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(1) FROM nyse_staging \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18b39c04-85e5-4b08-acfc-d50a1179f115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "338bf15e-6fea-40a9-99f6-4d26ebcdba74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|dateyear|\n",
      "+--------+\n",
      "|    1997|\n",
      "|    1997|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select  date_format(to_date(tradedate, 'yyyyMMdd'), 'yyyy') dateyear from nyse_staging limit 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea182-6563-422e-a1f0-672da0288887",
   "metadata": {},
   "source": [
    "#### creating table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16ef6508-b41f-483f-bdfc-aeb8cf721096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS nyse_eod_part (\n",
    "  stockticker STRING,\n",
    "  tradedate STRING,\n",
    "  openprice FLOAT,\n",
    "  highprice FLOAT,\n",
    "  lowprice FLOAT,\n",
    "  closeprice FLOAT,\n",
    "  volume INT\n",
    ") PARTITIONED BY (tradeyear INT)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75897348-99bb-44d7-ab24-7b499d1b5ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Describing the created partitioned table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1222015-f869-4e05-bb3d-407819ab24b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|stockticker                 |string                                                          |null   |\n",
      "|tradedate                   |string                                                          |null   |\n",
      "|openprice                   |float                                                           |null   |\n",
      "|highprice                   |float                                                           |null   |\n",
      "|lowprice                    |float                                                           |null   |\n",
      "|closeprice                  |float                                                           |null   |\n",
      "|volume                      |int                                                             |null   |\n",
      "|tradeyear                   |int                                                             |null   |\n",
      "|# Partition Information     |                                                                |       |\n",
      "|# col_name                  |data_type                                                       |comment|\n",
      "|tradeyear                   |int                                                             |null   |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Database                    |exercise                                                        |       |\n",
      "|Table                       |nyse_eod_part                                                   |       |\n",
      "|Owner                       |hadoop                                                          |       |\n",
      "|Created Time                |Tue Mar 28 04:13:19 UTC 2023                                    |       |\n",
      "|Last Access                 |UNKNOWN                                                         |       |\n",
      "|Created By                  |Spark 3.3.1                                                     |       |\n",
      "|Type                        |MANAGED                                                         |       |\n",
      "|Provider                    |hive                                                            |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1679976799]                              |       |\n",
      "|Location                    |hdfs://master:9000/user/hive/warehouse/exercise.db/nyse_eod_part|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe              |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                        |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat      |       |\n",
      "|Storage Properties          |[serialization.format=,, field.delim=,]                         |       |\n",
      "|Partition Provider          |Catalog                                                         |       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED nyse_eod_part\").show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5e512-1699-4831-a9c6-284fc6833431",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inserting data into partitioned table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d96361d-6643-491b-bca6-a30152ae3a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"INSERT INTO TABLE nyse_eod_part PARTITION (tradeyear)\n",
    "SELECT ns.*, cast(date_format(to_date(tradedate, 'yyyyMMdd'), 'yyyy')as int) tradeyear\n",
    "FROM nyse_staging ns\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e36ab-a5f8-4ec8-99de-da5796ea4bef",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95c5882a-083e-4f32-ab22-93d47ff058af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     partition|\n",
      "+--------------+\n",
      "|tradeyear=1997|\n",
      "|tradeyear=1998|\n",
      "|tradeyear=1999|\n",
      "|tradeyear=2000|\n",
      "|tradeyear=2001|\n",
      "|tradeyear=2002|\n",
      "|tradeyear=2003|\n",
      "|tradeyear=2004|\n",
      "|tradeyear=2005|\n",
      "|tradeyear=2006|\n",
      "|tradeyear=2007|\n",
      "|tradeyear=2008|\n",
      "|tradeyear=2009|\n",
      "|tradeyear=2010|\n",
      "|tradeyear=2011|\n",
      "|tradeyear=2012|\n",
      "|tradeyear=2013|\n",
      "|tradeyear=2014|\n",
      "|tradeyear=2015|\n",
      "|tradeyear=2016|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show partitions nyse_eod_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0f29ca9-6e2c-48fc-97f1-20b6f5cc9009",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 items\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=1997\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=1998\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=1999\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2000\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2001\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2002\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2003\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2004\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2005\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2006\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2007\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2008\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2009\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2010\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2011\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2012\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2013\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:14 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2014\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:15 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2015\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:15 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2016\n",
      "drwxrwxrwx   - hadoop supergroup          0 2023-03-28 04:15 /user/hive/warehouse/exercise.db/nyse_eod_part/tradeyear=2017\n"
     ]
    }
   ],
   "source": [
    " !hdfs dfs -ls /user/hive/warehouse/exercise.db/nyse_eod_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5525728f-5d52-43ba-a123-0820dd1024c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=====================================================>  (40 + 2) / 42]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 9384739|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(1) FROM nyse_eod_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1b5523e-09f8-469d-a154-00b9eafca614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63e8f7a1-a5ca-45d3-9bb4-e9d5400ac71c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9384739, 7)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'/home/hadoop/data/data/nyse_all/nyse_data' \n",
    "all_files = glob.glob(path + \"/*.txt.gz\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=None)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6db31-384b-47a1-b47a-4a46a1033730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
